<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title></title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Menu</div>
<div class="menu-item"><a href="index.html">About</a></div>
<div class="menu-item"><a href="papers.html" class="current">Papers</a></div>
<div class="menu-item"><a href="CV.pdf">CV</a></div>
</td>
<td id="layout-content">
<p><br /></p>
<h2>Theses</h2>
<ul>
<li><p>Learning Cooperation for Partially Observable Multi-Agent Path Finding <a href="https://qiushi-Lin.github.io/documents/lin2023thesis.pdf">[pdf]</a> <a href="https://qiushi-Lin.github.io/documents/lin2023defence.pdf">[slides]</a><br />
Advisor: <a href="https://www.cs.sfu.ca/~hangma">Hang Ma</a><br />
Committee: <a href="https://www.cs.sfu.ca/~oschulte">Oliver Schulte</a>, <a href="https://xbpeng.github.io/">Xue Bin (Jason) Peng</a></p>
</li>
</ul>
<h2>Publications</h2>
<ul>
<li><p><a href="https://doi.org/10.1109/LRA.2023.3292004">SACHA: Soft Actor-Critic with Heuristic-Based Attention for Partially Observable Multi-Agent Path Finding</a><br />
<b>Qiushi Lin</b>, Hang Ma<br />
IEEE Robotics and Automation Letters (RA-L) 2023 <a href="https://qiushi-Lin.github.io/documents/lin2023sacha.pdf">[pdf]</a> <a href="https://github.com/Qiushi-Lin/SACHA">[code]</a><br />
TL;DR: We designed a novel multi-agent actor-critic reinforcement framework for partially observable multi-agent path finding. We integrated the heuristic-based attention mechanisms to enable the learned model to generalize among multiple instances on a large scale.</p>
</li>
</ul>
<h2>Preprints</h2>
<ul>
<li><p>Mean Field Control with Envelope Q-learning for Moving Decentralized Agents in Formation<br />
<b>Qiushi Lin</b>, Hang Ma<br />
Preprint (In Submission) <a href="https://qiushi-Lin.github.io/documents/lin2023mfceq.pdf">[pdf]</a> <a href="https://github.com/Qiushi-Lin/MFCEQ">[code]</a><br />
TL;DR: We proposed an adaptable multi-objective multi-agent reinforcement learning algorithm that combines mean field control and envelop Q-learning for moving agents in formation, and provided theoretical analysis and empirical evaluation.</p>
</li>
</ul>
<h2>Reports</h2>
<p>(* = equal contribution)</p>
<ul>
<li><p>On the Convergence Rates of Log-Linear Policy Gradient Methods <a href="https://qiushi-Lin.github.io/documents/lin2023llpg.pdf">[pdf]</a> <a href="https://colab.research.google.com/drive/1WVkYJ68bW-lnFM9Jty4OwGV8J9afnAk8?usp=sharing">[code]</a><br />
<b>Qiushi Lin</b>*, Matin Aghaei*, Anderson de Andrade*, Sharan Vaswani<br />
TL;DR: We provided a general framework to derive convergence rates of policy gradient methods for log-linear policy class by reducing the problem to the one in tabular softmax settings. Based on this, we extended theoretical guarantees of softmax policy gradient methods to derive theoretically guaranteed algorithms for log-linear policies with both exact and inexact policy evaluation.</p>
</li>
</ul>
<ul>
<li><p>A Survey of Apprenticeship Learning <a href="https://qiushi-Lin.github.io/documents/lin2022apprenticeship.pdf">[pdf]</a><br />
<b>Qiushi Lin</b>*, Ziqian Bai*, Minh Bui*, Jiaqi Tan*<br />
TL;DR: We surveyed the literature on a few widely used apprenticeship learning algorithms and empirically evaluated them on a shared benchmark</p>
</li>
</ul>
</td>
</tr>
</table>
</body>
</html>
