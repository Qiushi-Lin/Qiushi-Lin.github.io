<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title></title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Menu</div>
<div class="menu-item"><a href="index.html">About</a></div>
<div class="menu-item"><a href="papers.html" class="current">Papers</a></div>
<div class="menu-item"><a href="cv.pdf">CV</a></div>
</td>
<td id="layout-content">
<p><br /></p>
<h2>Thesis</h2>
<ul>
<li><p>Learning Cooperation for Partially Observable Multi-Agent Path Finding <a href="https://qiushi-Lin.github.io/documents/lin2023thesis.pdf">[pdf]</a> <a href="https://qiushi-Lin.github.io/documents/lin2023defence.pdf">[slides]</a><br />
Advisor: Hang Ma
Committee: Oliver Schulte, Xue Bin Peng</p>
</li>
</ul>
<h2>Publications</h2>
<ul>
<li><p><a href="https://doi.org/10.1109/LRA.2023.3292004">SACHA: Soft Actor-Critic with Heuristic-Based Attention for Partially Observable Multi-Agent Path Finding</a><br />
<b>Qiushi Lin</b>, Hang Ma<br />
IEEE Robotics and Automation Letters (RA-L) 2023 <a href="https://qiushi-Lin.github.io/documents/lin2023sacha.pdf">[pdf]</a> <a href="https://github.com/Qiushi-Lin/SACHA">[code]</a><br />
TL;DR: We designed a novel multi-agent actor-critic reinforcement framework for partially observable multi-agent path finding. We integrated the heuristic-based attention mechanisms to enable the learned model to generalize among multiple instances on a large scale.</p>
</li>
</ul>
<h2>Preprints</h2>
<ul>
<li><p>Mean Field Control with Envelope Q-learning for Moving Agents in Formation<br />
<b>Qiushi Lin</b>, Hang Ma<br />
Preprint (In Submission) <a href="https://qiushi-Lin.github.io/documents/lin2023mfceq.pdf">[pdf]</a> <a href="https://github.com/Qiushi-Lin/MFCEQ">[code]</a><br />
TL;DR: We proposed an adaptable multi-objective multi-agent reinforcement learning algorithm that combines mean field control and envelop Q-learning for moving agents in formation, and provided theoretical analysis and empirical evaluation.</p>
</li>
</ul>
<h2>Reports</h2>
<ul>
<li><p>On the Convergence Rates of Log-Linear Policy Gradient Methods<br /> <a href="https://qiushi-Lin.github.io/documents/lin2023pg.pdf">[pdf]</a><br />
(by alphabetic order) Matin Aghaei, Anderson de Andrade, <b>Qiushi Lin</b></p>
</li>
</ul>
<ul>
<li><p>A Survey of Apprenticeship Learning<br /> <a href="https://qiushi-Lin.github.io/documents/lin2022apprenticeship.pdf">[pdf]</a><br />
(by alphabetic order) Ziqian Bai, Minh Bui, <b>Qiushi Lin</b>, Jiaqi Ta</p>
</li>
</ul>
</td>
</tr>
</table>
</body>
</html>
